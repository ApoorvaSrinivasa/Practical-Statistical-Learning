---
title: 'CS 598: Homework 2'
author: "Fall 2019, Apoorva (apoorva6)"
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
---

```{r setup, include=FALSE}
  knitr::opts_chunk$set(include = FALSE)  # TRUE for solution; FALSE for questions set

  knitr::opts_chunk$set(echo = TRUE)
  knitr::opts_chunk$set(message = FALSE)
  knitr::opts_chunk$set(warning = FALSE)
  knitr::opts_chunk$set(fig.height = 6, fig.width = 8, out.width = '50%', fig.align = "center")
  options(width = 90)
```

```{css, echo=FALSE}
.solution {
background-color: #e6ffe6;
}
```


## Question 1 Linear Model Selection

We will use the Boston Housing data again. This time, we do not scale the covariate. We will still remove `medv`, `town` and `tract` from the data and use `cmedv` as the outcome. If you do not use R, you can download a `.csv' file from the course website. 

```{r include = TRUE}
library(mlbench)
data(BostonHousing2)
BH = BostonHousing2[, !(colnames(BostonHousing2) %in% c("medv", "town", "tract"))]
linear_model <- lm(cmedv~., data = BH)
```

Answer the following questions:

a. Report the most significant variable from this full model with all features.

```{r include = TRUE}
p_value <- summary(linear_model)$coefficients[-1,4] 
p_value 
```

At a significance level of 0.05 we fail to reject the Null Hypothesis that there is no relationship between some of the predictors and the response. The following set of 11 predictors are the ones that define the response the most:
`r names(p_value[p_value< 0.05]) `

Out the this the most significant variable would be the **lstat** variable which has the lowest Pvalue among them all.


b. Starting from this full model, use stepwise regression with both forward and backward and BIC criterion to select the best model. Which variables are removed from the full model?

```{r ,results='hide'}
stepwise_model <- step(lm(BH$cmedv~., data = BH),direction="both",criterion="BIC")
```
```{r include = TRUE}
names(coef(linear_model))[!(names(coef(linear_model))%in% names(coef(stepwise_model)))]
```

The variables exculded from the model with the lowest BIC values are **`r names(coef(linear_model))[!(names(coef(linear_model))%in% names(coef(stepwise_model)))] `**


c.  Starting from this full model, use the best subset selection and list the best model of each model size. 
```{r include = TRUE}
library(leaps)
best_sub_selection <- regsubsets(BH$cmedv~., data = BH,nvmax =15)
summary(best_sub_selection)$outmat
```


d.  Use the Cp criterion to select the best model from part c). Which variables are removed from the full model? What is the most significant variable?

```{r include = TRUE}
lowest_cp <- which.min(summary(best_sub_selection)$cp)
best_pred <- names(coef(best_sub_selection,lowest_cp))
best_pred[-1]

# Building LM using the model chosen from best subset selection
best_model <- lm(BH$cmedv~ crim+zn+chas+nox+rm+dis+rad+tax+ptratio+b+lstat, data=BH)
coef(summary(best_model))[,4]
```

* The variables removed from the Full model after best subset selection are : **`r names(coef(linear_model))[!(names(coef(linear_model))%in% best_pred)] `**

* The most significant variable out of the 11 variables in the best subset model is **lstat** as it has the lowest P-value



## Question 2 Code Your Own Lasso

For this question, we will write our own Lasso code. You are not allowed to use any built-in package that already implements Lasso. First, we will generate simulated data. Here, only $X_1$, $X_2$ and $X_3$ are important, and we will not consider the intercept term. 

```{r include = TRUE}
library(MASS)
set.seed(1)
n = 200
p = 200
# generate data
V = matrix(0.2, p, p)
diag(V) = 1
X = as.matrix(mvrnorm(n, mu = rep(0, p), Sigma = V))
y = X[, 1] + 0.5*X[, 2] + 0.25*X[, 3] + rnorm(n)
# we will use a scaled version
X = scale(X)
y = scale(y)

```

As we already know, coordinate descent is an efficient approach for solving Lasso. The algorithm works by updating one parameter at a time, and loop around all parameters until convergence. 

a. Hence, we need first to write a function that updates just one parameter, which is also known as the soft-thresholding function. Construct the function in the form of `soft_th <- function(b, lambda)`, where `b` is a number that represents the one-dimensional linear regression solution, and `lambda` is the penalty level. The function should output a scaler, which is the minimizer of 
$$(x - b)^2 + \lambda |b|$$

```{r include = TRUE}
soft_th <- function(b,lambda)
{
  soft_th_values <- lambda/2
  ifelse(b > soft_th_values, (b - soft_th_values) , ifelse( b < -(soft_th_values), (b+soft_th_values),0))
}
```

b.  Now lets pretend that at an iteration, the current parameter $\boldsymbol \beta$ value is given below (as `beta_old`, i.e., $\boldsymbol \beta^{\text{old}}$). Apply the above soft-thresholding function to update all $p$ parameters sequencially one by one to complete one "loop" of the updating scheme. Please note that we use the Gauss-Seidel style coordinate descent, in which the update of the next parameter is based on the new values of previous entries. Hence, each time a parameter is updated, you should re-calculate the residual 
$$\mathbf{r} = \mathbf{y} - \mathbf{X}^{\text T} \boldsymbol \beta$$ 
so that the next parameter update reflects this change. After completing this one enrire loop, print out the first 3 observations of $\mathbf{r}$ and the nonzero entries in the updated $\boldsymbol \beta^{\text{new}}$ vector. For this question, use `lambda` = 0.7 and


```{r include = TRUE}

beta_old = rep(0, p)
 lambda = 0.7

 for(j in 1:p)
 {
   beta_star <- (t(X[,j]) %*% (y - (X[,-j] %*% beta_old[-j]))) / ( t(X[,j]) %*% X[,j])
   beta_old[j] <-  soft_th(beta_star,lambda)
 }
 
 r <- y - (X %*% beta_old)
```

 * The first 3 observations of $\mathbf{r}$ : `r r[1:3] `
 * The nonzero entries in the updated $\boldsymbol \beta^{\text{new}}$ vector : `r beta_old[beta_old != 0]`
 
 
c. Now, let us finish the entire Lasso algorithm. We will write a function `myLasso(X, y, lambda, tol, maxitr)`. Set the tolerance level `tol` = 1e-5, and `maxitr` = 100 as the default value. Use the "one loop" code that you just wrote in the previous question, and integrate that into a grand for-loop that will continue updating the parameters up to `maxitr` runs. Check your parameter updates once in this grand loop and stop the algorithm once the $\ell_1$ distance between $\boldsymbol \beta^{\text{new}}$ and $\boldsymbol \beta^{\text{old}}$ is smaller than `tol`. Use `  beta_old = rep(0, p)` as the initial value, and `lambda` = 0.3. After the algorithm converges, report the following: i) the number of iterations took; ii) the nonzero entries in the final beta parameter estimate, and iii) the first three observations of the residual. Please write your algorithm as efficient as possible.


```{r include = TRUE}


myLasso <- function(X, y, lambda, tol, maxitr)
{
 iter <- 0
 beta_new = rep(0, p)
 beta_old = rep(0, p)
diff_l1 <- 10

while (diff_l1 > tol & iter < maxitr){
for(j in 1:p)
 {
   beta_star <- (t(X[,j]) %*% (y - (X[,-j] %*% beta_old[-j]))) / ( t(X[,j]) %*% X[,j])
   beta_old[j] <-  soft_th(beta_star,lambda)
 }
 
 diff_l1 <- sum(abs(beta_new-beta_old))
 beta_new <- beta_old
 iter <- iter + 1
 
}

 r <- y - (X %*% beta_old)
 return(list("iter" = iter, "r"= r[1:3], "nonzer_beta"= beta_new[beta_new!=0]))
 
}


#Calling function
out <- myLasso(X, y, 0.3,  1e-5, 100)
out

```


d. Now we have our own Lasso function, let's check the result and compare it with the `glmnet` package. Note that for the glmnet package, their `lambda` should be set as half of ours. Comment on the accuracy of the algorithm that we wrote. Please note that the distance of the two solutions should not be larger than 0.005.

```{r include = TRUE}
# Lasso using glmnet
library(glmnet)
lasso = glmnet(X, y,alpha =1,lambda = 0.15,thresh=1e-5)
nonzero_beta_lasso <-  lasso$beta[lasso$beta !=0]

# Checking the difference in the betas between myLasso and glmnet
distance_betas <- sum(abs(nonzero_beta_lasso-out[[3]]))
distance_betas 
```
The MSE of our model is 0.3758599. Our algorithm results are very close to the prediction values of coefficients that we get from the library function glmnet. 



## Question 3 Cross-Validation for Model Selection

We will use the [Walmart Sales data](https://www.kaggle.com/anshg98/walmart-sales#Train.csv) provided on Kaggle. For this question, we will use only the Train.csv file. The file is also available at [here](https://teazrq.github.io/stat432/homework.html). 

a.  Do the following to process the data:
    + Read data into R
    + Convert character variables into factors
    + Remove `Item_Identifier`
    + Further convert all factors into dummy variables

```{r class.source="solution"}
  # Readin data
  library(tidyverse)
  WalmartSales <- read_csv("WalmartSalesTrain.csv")
  
  # find character variables
  char <- c("Item_Fat_Content", "Item_Type", "Outlet_Identifier", 
            "Outlet_Size", "Outlet_Location_Type", "Outlet_Type")
  WalmartSales[char] = lapply(WalmartSales[char], factor)
  
  # convert factors into dummies
  WalMartData <- model.matrix( ~ . -1, data = WalmartSales[, -c(1)])
```


b. Use all variables to model the outcome `Item_Outlet_Sales` in its $log$ scale. First, we randomly split the data into two parts with equal size. Make sure that you set a random seed so that the result can be replicated. Treat one as the training data, and the other one as the testing data. For the training data, perform the following:
    + Use cross-validation to select the best Lasso model. Consider both `lambda.min` and `lambda.1se`. Provide additional information to summarize the model fitting result
    + Use cross-validation to select the best Ridge model. Consider both `lambda.min` and `lambda.1se`. Provide additional information to summarize the model fitting result
    + Test these four models on the testing data and report and compare the prediction accuracy
    
```{r include = TRUE}
#install.packages("glmnet")
library(glmnet)
#Split data into test and train
set.seed(1)
train_ind <- sample(nrow(WalMartData), nrow(WalMartData)/2)

train <- WalMartData[train_ind, ]
test <- WalMartData[-train_ind, ]

########## Lasso model ###########
lasso_train = cv.glmnet(train[,-(ncol(train))], log(train[,ncol(train)]))
lasso_lam_min <- lasso_train$lambda.min
lasso_lam_1se <- lasso_train$lambda.1se
plot(lasso_train)
```

```{r include = TRUE}
########## Ridge model ###########
ridge_train = cv.glmnet(train[,-(ncol(train))], log(train[,(ncol(train))]), alpha = 0)
ridge_lam_min <- ridge_train$lambda.min
ridge_lam_1se <- ridge_train$lambda.1se
plot(ridge_train)
```

```{r include = TRUE}
#Predicting on test dataset
# Lasso with min lamda
lasso_test_lammin = predict(lasso_train, s =lasso_lam_min, newx= test[,-(ncol(test))])
acc_min_lasso <- mean((lasso_test_lammin - log(test[,ncol(test)]))^2)
# Lasso with 1se lamda
lasso_test_lam1se = predict(lasso_train, s =lasso_lam_1se, newx= test[,-(ncol(test))])
acc_1se_lasso <- mean((lasso_test_lam1se - log(test[,ncol(test)]))^2)
# Ridge with min lamda
ridge_test_lammin = predict(ridge_train, s =ridge_lam_min, newx= test[,-(ncol(test))])
acc_min_ridge <- mean((ridge_test_lammin - log(test[,ncol(test)]))^2)
# Ridge with 1se lamda
ridge_test_lam1se = predict(ridge_train, s =ridge_lam_1se, newx= test[,-(ncol(test))])
acc_1se_ridge <- mean((ridge_test_lam1se - log(test[,ncol(test)]))^2)

MSE_accuracies <- c(acc_min_lasso,acc_1se_lasso,acc_min_ridge,acc_1se_ridge)
names(MSE_accuracies) <- c("Lasso_minLambda","Lasso_1seLambda","Ridge_minLamda","Ridge_1seLamda")
MSE_accuracies
```
The Lasso model using min lamda performs the best among the four models above.

 * Using lambda.min as the best lambda for lasso model, gives the following regression coefficients:
```{r include = TRUE}
coef(lasso_train, lasso_train$lambda.min)
```

 * Using lambda.1se as the best lambda for lasso model, gives the following regression coefficients:
```{r include = TRUE}
coef(lasso_train, lasso_train$lambda.1se) 
```

 * Using lambda.min as the best lambda for ridge model, gives the following regression coefficients:
```{r include = TRUE}
coef(ridge_train, ridge_train$lambda.min) 
```

 * Using lambda.1se as the best lambda for ridge model, gives the following regression coefficients:
```{r include = TRUE} 
coef(ridge_train, ridge_train$lambda.1se) 
```

This shows that the lasso regression eliminates more predictors by setting them to 0.
